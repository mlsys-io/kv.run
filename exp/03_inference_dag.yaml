apiVersion: mloc/v1
kind: InferenceTask
metadata:
  name: llama31-8b-dag
  owner: demo
  annotations:
    description: "Two-branch DAG inference over GSM8K with multi-GPU vLLM"

spec:
  taskType: "inference"

  resources:
    replicas: 1
    hardware:
      cpu: "16"
      memory: "64Gi"
      gpu:
        type: "any"
        count: 1

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.1-8B-Instruct"
      revision: "main"
    vllm:
      gpu_memory_utilization: 0.9
      trust_remote_code: true

  graph:
    nodes:
      - name: branch-a
        spec:
          taskType: "inference"

          data:
            type: "dataset"
            url: "openai/gsm8k"
            name: "main"
            split: "train[:100]"
            column: "question"
            shuffle: false

          inference:
            system_prompt: |
              Produce a high-level reasoning outline before answering the math problem.
            max_tokens: 192
            temperature: 0.25
            top_p: 0.9

      - name: branch-b
        spec:
          taskType: "inference"

          data:
            type: "dataset"
            url: "openai/gsm8k"
            name: "main"
            split: "train[:100]"
            column: "question"
            shuffle: false

          inference:
            system_prompt: |
              Verify the draft solution for the same problem and report a concise numeric answer with confidence.
            max_tokens: 128
            temperature: 0.15
            top_p: 0.85

      - name: synthesis
        dependsOn:
          - "branch-a"
          - "branch-b"
        spec:
          taskType: "inference"

          data:
            type: "graph_template"
            template:
              name: "two_column_briefing"
              columns:
                - label: "Reasoning drafts"
                  expr: "branch-a.result.items"
                - label: "Direct answers"
                  expr: "branch-b.result.items"
              options:
                intro:
                  - "You are reviewing two inference passes run on the same 100 GSM8K questions."
                  - "Column A contains structured reasoning drafts; Column B contains verified final answers."
                closing:
                  - "Compare the approaches, flag any mismatched answers, and produce guidance for a unified response style."

          inference:
            system_prompt: |
              Combine the outline and direct answer into a structured explanation with a final answer.
            max_tokens: 256
            temperature: 0.2
            top_p: 0.85

  output:
    destination:
      type: "http"
      url: "http://localhost:8000/api/v1/results"
    artifacts:
      - "responses.json"
      - "logs"
