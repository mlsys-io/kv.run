# 使用 LoRA 对 llama 1B 在 GSM8K 上做快速适配，并立刻加载合并权重在 GSM8K 上进行推理评估。
apiVersion: mloc/v1
kind: LoRASFTTask
metadata:
  name: llama1b-lora-transfer-eval
  owner: demo
  annotations:
    description: "LoRA adaptation on GSM8K followed by merged-weight inference on GSM8K"

spec:
  taskType: "lora_sft"

  resources:
    replicas: 1
    hardware:
      cpu: "12"
      memory: "48Gi"
      gpu:
        type: "any"
        count: 1
        memory: "48Gi"

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-1B-Instruct"
      revision: "main"

  stages:
    - name: lora-train
      spec:
        taskType: "lora_sft"
        lora:
          r: 16
          alpha: 32
          dropout: 0.05
          bias: "none"
          target_modules:
            - "q_proj"
            - "k_proj"
            - "v_proj"
            - "o_proj"
          task_type: "CAUSAL_LM"
        data:
          dataset_name: "openai/gsm8k"
          config_name: "main"
          split: "train[:1%]"
          prompt_column: "question"
          response_column: "answer"
          separator: "\n\nReference answer: "
          max_samples: 200
        training:
          num_train_epochs: 1
          batch_size: 2
          gradient_accumulation_steps: 4
          learning_rate: 2.0e-4
          max_seq_length: 512
          logging_steps: 20
          save_steps: 80
          save_model: true
          gradient_checkpointing: false
          packing: false
          fp16: true
        output:
          destination:
            type: "http"
            url: "http://localhost:8000/api/v1/results"
            timeoutSec: 30
          artifacts:
            - "responses.json"
            - "final_lora"
            - "logs"

    - name: gsm8k-inference
      spec:
        taskType: "inference"
        model:
          adapters:
            - type: "lora"
              path: "${lora-train.result.final_lora_path}"
              taskId: "${lora-train.result.task_id}"
              url: "${lora-train.result.final_lora_archive_url}"
              apply: "merge"
        data:
          type: "dataset"
          url: "openai/gsm8k"
          name: "main"
          split: "train[:200]"
          column: "question"
          shuffle: true
          seed: 11
        inference:
          system_prompt: |
            Apply the math-specialist LoRA adapter to solve the word problem step by step and end with "Answer: <value>".
          max_tokens: 256
          temperature: 0.2
          top_p: 0.9
        output:
          destination:
            type: "http"
            url: "http://localhost:8000/api/v1/results"
            timeoutSec: 30
          artifacts:
            - "responses.json"
            - "logs"
