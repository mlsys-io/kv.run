# 用 PPO 对 llama 1B 在短指令集上强化其正向响应，再依据评估提示在 MT-Bench 上执行推理检验。
apiVersion: mloc/v1
kind: TrainingTask
metadata:
  name: llama1b-ppo-mtbench-loop
  owner: demo
  annotations:
    description: "Compact PPO alignment followed by MT-Bench style inference using evaluation guidance"

spec:
  taskType: "ppo"

  resources:
    replicas: 1
    hardware:
      cpu: "12"
      memory: "48Gi"
      gpu:
        type: "any"
        count: 1
        memory: "48Gi"

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-1B-Instruct"
      revision: "main"
      trust_remote_code: false
    config:
      fp16: true
      device_map_auto: true

  stages:
    - name: ppo-train
      spec:
        taskType: "ppo"
        reward_model:
          identifier: "cardiffnlp/twitter-roberta-base-sentiment-latest"
          type: "sentiment"
        data:
          prompts:
            - "Write a concise and uplifting response to: I am nervous about my presentation tomorrow."
            - "Give an encouraging explanation of how transformers models work."
            - "Provide constructive advice for balancing study and rest."
            - "Create a cheerful message celebrating a small win."
            - "Offer a motivating reminder to keep learning every day."
        training:
          learning_rate: 1.41e-5
          batch_size: 1
          per_device_train_batch_size: 1
          mini_batch_size: 1
          gradient_accumulation_steps: 1
          steps: 2
          ppo_epochs: 1
          target_kl: 0.1
          seed: 42
          save_model: true
          save_freq: 32
        generation:
          max_new_tokens: 16
          temperature: 0.7
          do_sample: true
        output:
          destination:
            type: "http"
            url: "http://localhost:8000/api/v1/results"
          artifacts:
            - "responses.json"
            - "checkpoints"
            - "logs"

    - name: evaluation-notes
      spec:
        taskType: "inference"
        model:
          vllm:
            gpu_memory_utilization: 0.50
            trust_remote_code: true
        data:
          type: "graph_template"
          template:
            name: "two_column_briefing"
            columns:
              - label: "Training metrics"
                expr: "ppo-train.result"
              - label: "Checkpoint directory"
                expr: "ppo-train.result.output_dir"
            options:
              intro:
                - "You are documenting how to validate the PPO-aligned model."
              closing:
                - "Return a short checklist covering checkpoint activation, safety checks, and MT-Bench scoring tips."
        inference:
          system_prompt: |
            Summarize the PPO run, list concrete evaluation steps, and highlight what qualitative traits to watch for.
          max_tokens: 256
          temperature: 0.2
          top_p: 0.85
        output:
          destination:
            type: "http"
            url: "http://localhost:8000/api/v1/results"
            timeoutSec: 30
          artifacts:
            - "responses.json"
            - "logs"

    - name: mtbench-run
      spec:
        taskType: "inference"
        data:
          type: "dataset"
          url: "dim/mt_bench_en"
          trust_remote_code: false
          split: "train[:60]"
          column: "turns"
          shuffle: true
          seed: 7
        inference:
          system_prompt: |
            Treat each prompt string as a conversation starter. Deliver a single high-quality assistant reply while following the PPO evaluation checklist.
          max_tokens: 256
          temperature: 0.2
          top_p: 0.9
        output:
          destination:
            type: "http"
            url: "http://localhost:8000/api/v1/results"
            timeoutSec: 30
          artifacts:
            - "responses.json"
            - "logs"
