apiVersion: mloc/v1
kind: TrainingTask
metadata:
  name: llama32-3b-ppo
  owner: demo
  annotations:
    description: "PPO fine-tuning on GSM8K prompts with reward based on solution quality"

spec:
  taskType: "ppo"

  resources:
    replicas: 1
    hardware:
      cpu: "16"
      memory: "64Gi"
      gpu:
        type: "any"
        count: 2
        memory: "48Gi"

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-3B-Instruct"
      revision: "main"
    config:
      fp16: true
      device_map_auto: true

  reward_model:
    identifier: "openai-community/gsm8k_cot_reward"
    type: "classification"

  data:
    dataset_name: "openai/gsm8k"
    config_name: "main"
    split: "train[:100]"
    prompt_column: "question"

  training:
    learning_rate: 1.0e-5
    batch_size: 4
    mini_batch_size: 2
    gradient_accumulation_steps: 4
    steps: 100
    ppo_epochs: 4
    target_kl: 0.1
    seed: 42
    save_model: true
    save_freq: 50
    optimize_cuda_cache: true

  generation:
    max_new_tokens: 192
    temperature: 0.7
    do_sample: true

  output:
    destination:
      type: "local"
      path: "./runs/08_ppo_training"
    artifacts:
      - "responses.json"
      - "trained_model"
      - "logs"
