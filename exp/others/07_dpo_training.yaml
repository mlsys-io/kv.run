apiVersion: mloc/v1
kind: TrainingTask
metadata:
  name: llama32-3b-dpo
  owner: demo
  annotations:
    description: "Preference optimization on GSM8K-inspired prompts (100-sample budget)"

spec:
  taskType: "dpo"

  resources:
    replicas: 1
    hardware:
      cpu: "16"
      memory: "64Gi"
      gpu:
        type: "any"
        count: 2
        memory: "46Gi"

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-3B-Instruct"
      revision: "main"
    config:
      fp16: true
      device_map_auto: true

  data:
    notes: "Prompts adapted from openai/gsm8k train split; keep total <=100 when extending."
    preferences:
      - prompt: "A classroom has 24 students. If they are arranged into groups of 6, how many groups are formed?"
        chosen: "There are 24 / 6 = 4 groups formed."
        rejected: "It is impossible to know since the group size is not given."
      - prompt: "Lena buys 3 notebooks for $2 each and a pen for $1. How much does she spend?"
        chosen: "Three notebooks cost 3 x 2 = 6 dollars; adding the pen gives 6 + 1 = 7 dollars."
        rejected: "The total is $6 because the pen was probably on sale."
      - prompt: "The sum of two numbers is 19 and one number is 8. What is the other number?"
        chosen: "19 - 8 = 11, so the remaining number is 11."
        rejected: "Since 8 is given, the other number must also be 8."
      - prompt: "A train travels 45 miles per hour for 3 hours. What distance does it cover?"
        chosen: "Distance = 45 x 3 = 135 miles."
        rejected: "It travels 90 miles because trains slow down halfway."

  training:
    learning_rate: 1.0e-6
    batch_size: 4
    gradient_accumulation_steps: 8
    num_train_epochs: 1
    beta: 0.1
    save_model: true
    save_freq: 50

  output:
    destination:
      type: "local"
      path: "./runs/07_dpo_training"
    artifacts:
      - "responses.json"
      - "trained_model"
      - "logs"
