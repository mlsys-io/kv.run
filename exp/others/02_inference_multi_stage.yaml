apiVersion: mloc/v1
kind: InferenceTask
metadata:
  name: llama31-8b-multi-stage
  owner: demo
  annotations:
    description: "Draft, refine, and structure GSM8K answers with staged vLLM inference"

spec:
  taskType: "inference"

  resources:
    replicas: 1
    hardware:
      cpu: "16"
      memory: "64Gi"
      gpu:
        type: "any"
        count: 1
        memory: "22Gi"

  data:
    type: "dataset"
    url: "openai/gsm8k"
    name: "main"
    split: "train[:200]"
    column: "question"
    shuffle: true
    seed: 13

  inference:
    max_tokens: 256
    temperature: 0.4
    top_p: 0.9

  stages:
    - name: "draft-solver"
      spec:
        taskType: "inference"

        resources:
          replicas: 1
          hardware:
            cpu: "16"
            memory: "64Gi"
            gpu:
              type: "any"
              count: 1
              memory: "22Gi"

        model:
          source:
            type: "huggingface"
            identifier: "meta-llama/Llama-3.1-8B-Instruct"
            revision: "main"
          vllm:
            gpu_memory_utilization: 0.9
            trust_remote_code: true

        inference:
          system_prompt: |
            Solve the math problem carefully. Produce a structured draft with labeled steps.
          temperature: 0.3
          top_p: 0.9

        output:
          destination:
            type: "http"
            url: "http://localhost:8000/api/v1/results"
          artifacts:
            - "responses.json"
            - "logs"

    - name: "refine-explainer"
      spec:
        taskType: "inference"

        resources:
          replicas: 1
          hardware:
            cpu: "16"
            memory: "64Gi"
            gpu:
              type: "any"
              count: 2
              memory: "22Gi"

        model:
          source:
            type: "huggingface"
            identifier: "meta-llama/Llama-3.1-8B-Instruct"
            revision: "main"
          vllm:
            gpu_memory_utilization: 0.9
            trust_remote_code: true

        inference:
          system_prompt: |
            Improve the draft so it is concise, accurate, and easy to follow. Preserve the final answer.
          temperature: 0.2
          top_p: 0.85

        output:
          destination:
            type: "http"
            url: "http://localhost:8000/api/v1/results"
          artifacts:
            - "responses.json"
            - "logs"

    - name: "json-formatter"
      spec:
        taskType: "inference"

        resources:
          replicas: 1
          hardware:
            cpu: "16"
            memory: "64Gi"
            gpu:
              type: "any"
              count: 2
              memory: "22Gi"

        model:
          source:
            type: "huggingface"
            identifier: "meta-llama/Meta-Llama-3.1-8B-Instruct"
            revision: "main"
          vllm:
            gpu_memory_utilization: 0.9
            trust_remote_code: true

        inference:
          system_prompt: |
            Rewrite the refined solution as strict JSON with fields question, answer, rationale, confidence (0-1).
          temperature: 0.1
          top_p: 0.8

        output:
          destination:
            type: "http"
            url: "http://localhost:8000/api/v1/results"
          artifacts:
            - "responses.json"
            - "logs"
