# --- Stage 1: Train a LoRA adapter on GSM8K (100 samples)
apiVersion: mloc/v1
kind: LoRASFTTask
metadata:
  name: llama32-3b-lora-prepare
  owner: demo
  annotations:
    description: "LoRA adapter training on GSM8K before downstream inference"

spec:
  taskType: "lora_sft"

  resources:
    replicas: 1
    hardware:
      cpu: "16"
      memory: "64Gi"
      gpu:
        type: "any"
        count: 2

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-3B-Instruct"
      revision: "main"

  lora:
    r: 16
    alpha: 32
    dropout: 0.05
    bias: "none"
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
    task_type: "CAUSAL_LM"

  data:
    dataset_name: "openai/gsm8k"
    config_name: "main"
    split: "train[:100]"
    prompt_column: "question"
    response_column: "answer"
    separator: "\n\nAnswer: "

  training:
    allow_multi_gpu: true
    visible_devices: "0,1"
    num_train_epochs: 1
    batch_size: 2
    gradient_accumulation_steps: 4
    learning_rate: 2.0e-4
    max_seq_length: 512
    logging_steps: 5
    save_steps: 40
    save_model: true
    gradient_checkpointing: true
    packing: false
    bf16: true

  output:
    destination:
      type: "local"
      path: "./runs/11_lora_sft_plus_inference/lora"
    artifacts:
      - "responses.json"
      - "final_lora"
      - "logs"
---
# --- Stage 2: Run vLLM inference with the trained LoRA adapter (200 samples)
apiVersion: mloc/v1
kind: InferenceTask
metadata:
  name: llama31-8b-infer-with-lora
  owner: demo
  annotations:
    description: "Apply the trained LoRA adapter during vLLM inference on GSM8K"

spec:
  taskType: "inference"

  resources:
    replicas: 1
    hardware:
      cpu: "16"
      memory: "64Gi"
      gpu:
        type: "any"
        count: 2

  parallel:
    enabled: true
    max_shards: 2

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Meta-Llama-3.1-8B-Instruct"
      revision: "main"
    adapters:
      - type: "lora"
        url: "${PREVIOUS_TASK.final_lora_archive_url}"
        apply: "merge"
    vllm:
      tensor_parallel_size: 2
      gpu_memory_utilization: 0.9
      trust_remote_code: true

  data:
    type: "dataset"
    url: "openai/gsm8k"
    name: "main"
    split: "train[:200]"
    column: "question"
    shuffle: true
    seed: 99

  inference:
    system_prompt: |
      Use the adapter-trained reasoning style to solve the math word problem accurately.
    max_tokens: 256
    temperature: 0.2
    top_p: 0.9

  output:
    destination:
      type: "local"
      path: "./runs/11_lora_sft_plus_inference/inference"
    artifacts:
      - "responses.json"
      - "logs"
