# 先运行 llama 1B 的轻量 DPO 偏好对齐，再生成评估计划并在 TruthfulQA 上执行一次推理评测。
apiVersion: mloc/v1
kind: TrainingTask
metadata:
  name: llama1b-dpo-eval-loop
  owner: demo
  annotations:
    description: "Preference alignment with DPO followed by automatic evaluation briefing and factual QA run"

spec:
  taskType: "dpo"

  resources:
    replicas: 1
    hardware:
      cpu: "12"
      memory: "48Gi"
      gpu:
        type: "any"
        count: 1

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-1B-Instruct"
      revision: "main"
      trust_remote_code: false
    config:
      fp16: true
      device_map_auto: true

  stages:
    - name: dpo-align
      spec:
        taskType: "dpo"
        data:
          dataset_name: "HuggingFaceH4/ultrafeedback_binarized"
          config_name: "train_prefs"
          split: "train[:512]"
          max_samples: 256
        training:
          learning_rate: 5.0e-7
          batch_size: 4
          gradient_accumulation_steps: 2
          num_train_epochs: 1
          save_model: true
          save_freq: 200
        output:
          destination:
            type: "local"
            path: "./runs/composite_llama1b/05_dpo/dpo_align"
          artifacts:
            - "responses.json"
            - "checkpoints"
            - "logs"

    - name: evaluation-brief
      spec:
        taskType: "inference"
        data:
          type: "graph_template"
          template:
            name: "two_column_briefing"
            columns:
              - label: "Training summary"
                expr: "dpo-align.result"
              - label: "Checkpoint path"
                expr: "dpo-align.result.output_dir"
            options:
              intro:
                - "You are planning a post-alignment factuality evaluation."
                - "Use the training metadata to outline what to test."
              closing:
                - "Return a bullet list of evaluation goals and any manual steps required to swap in the aligned weights."
        inference:
          system_prompt: |
            Draft an evaluation checklist describing how to swap the newly aligned weights, what benchmarks to run, and how to compare against the base model.
          max_tokens: 256
          temperature: 0.2
          top_p: 0.85
        output:
          destination:
            type: "local"
            path: "./runs/composite_llama1b/05_dpo/eval_plan"
          artifacts:
            - "responses.json"
            - "logs"

    - name: truthfulqa-run
      spec:
        taskType: "inference"
        data:
          type: "dataset"
          url: "truthfulqa/truthful_qa"
          split: "validation[:200]"
          column: "question"
          shuffle: true
          seed: 314
        inference:
          system_prompt: |
            Assume the aligned weights have been loaded. Follow the evaluation checklist recommendations and answer each question truthfully.
          max_tokens: 192
          temperature: 0.15
          top_p: 0.85
        output:
          destination:
            type: "local"
            path: "./runs/composite_llama1b/05_dpo/truthfulqa"
          artifacts:
            - "responses.json"
            - "logs"
