apiVersion: mloc/v1
kind: LoRASFTTask
metadata:
  name: llama32-3b-lora-multiturn
  owner: demo
  annotations:
    description: "Two-stage LoRA fine-tuning to simulate multi-turn tutoring on GSM8K"

spec:
  taskType: "lora_sft"

  resources:
    replicas: 1
    hardware:
      cpu: "16"
      memory: "64Gi"
      gpu:
        type: "any"
        count: 1

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-3B-Instruct"
      revision: "main"

  lora:
    r: 16
    alpha: 32
    dropout: 0.05
    bias: "none"
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "o_proj"
    task_type: "CAUSAL_LM"

  output:
    destination:
      type: "http"
      url: "http://localhost:8000/api/v1/results"
    artifacts:
      - "responses.json"
      - "final_lora"
      - "logs"

  stages:
    - name: tutor-first-turn
      spec:
        taskType: "lora_sft"
        data:
          dataset_name: "openai/gsm8k"
          config_name: "main"
          split: "train[:60]"
          prompt_column: "question"
          response_column: "answer"
          separator: "\n\nTutor Turn 1: "
        training:
          allow_multi_gpu: false
          num_train_epochs: 1
          batch_size: 2
          gradient_accumulation_steps: 4
          learning_rate: 2.5e-4
          max_seq_length: 512
          logging_steps: 5
          save_steps: 30
          save_model: true
          gradient_checkpointing: true
          packing: false
          bf16: true

    - name: tutor-followup-turn
      spec:
        taskType: "lora_sft"
        checkpoint:
          load:
            type: "http"
            url: "${tutor-first-turn.result.final_lora_archive_url}"
        data:
          dataset_name: "openai/gsm8k"
          config_name: "main"
          split: "train[60:100]"
          prompt_column: "question"
          response_column: "answer"
          separator: "\n\nTutor Turn 2: "
        training:
          allow_multi_gpu: false
          num_train_epochs: 1
          batch_size: 2
          gradient_accumulation_steps: 4
          learning_rate: 2.0e-4
          max_seq_length: 512
          logging_steps: 5
          save_steps: 40
          save_model: true
          gradient_checkpointing: true
          packing: false
          bf16: true
