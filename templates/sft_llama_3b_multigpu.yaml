apiVersion: mloc/v1
kind: SFTTask
metadata:
  name: llama-3b-sft-multigpu

spec:
  taskType: "sft"

  resources:
    replicas: 1
    hardware:
      cpu: "8"
      memory: "32GiB"
      gpu:
        count: 2           # 请求 2 张 GPU（可根据机器修改）
        type: "any"

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-3B-Instruct"

  data:
    dataset_name: "openai/gsm8k"
    config_name: "main"
    split: "train[:5%]"
    prompt_column: "question"
    response_column: "answer"
    separator: "\n\nAnswer: "
    max_samples: 200

  training:
    # 让执行器内部使用 torchrun 启动多卡（无需用 accelerate/torchrun 启动整个 worker）
    allow_multi_gpu: true
    visible_devices: "0,1"     # 设置本任务可见的 GPU（按需调整）
    nproc_per_node: 2           # 进程数 = 使用的 GPU 数

    # 训练超参
    num_train_epochs: 2
    batch_size: 2
    gradient_accumulation_steps: 4
    learning_rate: 5.0e-5
    max_seq_length: 512
    logging_steps: 10
    save_steps: 100
    save_strategy: "steps"
    save_model: true

    # 稳定性/显存
    gradient_checkpointing: false  # full_shard 下建议关闭，改用 activation_checkpointing
    packing: false
    fp16: false
    bf16: true

    # FSDP（标准多卡配置）
    fsdp: "full_shard auto_wrap"
    fsdp_min_num_params: 0          # 避免与 transformer_layer_cls_to_wrap 互斥
    fsdp_config:
      activation_checkpointing: true
      transformer_layer_cls_to_wrap:
        - LlamaDecoderLayer
      sync_module_states: true
      forward_prefetch: true
      limit_all_gathers: true
      use_orig_params: true

  output:
    destination:
      type: "local"
      path: "./sft_llama_3b_multigpu"
    artifacts:
      - "responses.json"
      - "final_model"

