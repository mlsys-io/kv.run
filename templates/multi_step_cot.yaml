apiVersion: mloc/v1
kind: InferenceTask
metadata:
  name: multistage-infer
  owner: alice
  annotations:
    description: "Multi-stage inference: draft -> refine -> format"

spec:
  # The scheduler treats this as an inference pipeline (no data-parallel enabled).
  taskType: "inference"

  # Common defaults (can be overridden per stage).
  resources:
    replicas: 1
    hardware:
      cpu: "8"
      memory: "16Gi"
      # gpu:            # Default to CPU; GPU stages will override.
      #   type: "any"
      #   count: 0

  # Shared dataset config (stages can override split/column if needed).
  data:
    type: "dataset"
    url: "openai/gsm8k"
    name: "main"
    split: "train[:1%]"
    column: "question"
    shuffle: true
    seed: 42

  # Shared inference defaults (stages override as needed).
  inference:
    max_tokens: 128
    temperature: 0.7
    top_p: 0.95

  # Pipeline stages run sequentially; each depends on the previous stage.
  stages:

    # ---------- Stage 1: Draft reasoning (small CPU model, CoT-style draft) ----------
    - name: "draft-cot"
      spec:
        resources:
          hardware:
            cpu: "8"
            memory: "16Gi"
            # CPU-only (omit gpu or set count: 0)

        model:
          source:
            type: "huggingface"
            identifier: "meta-llama/Llama-3.2-1B-Instruct"
            revision: "main"
          transformers:
            device_map: "cpu"
            dtype: "float32"
            trust_remote_code: false
            low_cpu_mem_usage: true

        inference:
          # Stage-specific system prompt
          system_prompt: |
            You are a careful reasoning assistant. Produce a brief, step-by-step draft
            explaining how to approach the problem. Keep it structured and concise.
          temperature: 0.5
          top_p: 0.9

        output:
          destination:
            type: "local"
            path: "/ignored/by/current-worker"
          artifacts:
            - "responses.json"
            - "logs"

    # ---------- Stage 2: Refine answer (strong GPU model, edits the draft) ----------
    - name: "refine-gpu"
      spec:
        resources:
          hardware:
            cpu: "8"
            memory: "32Gi"
            gpu:
              type: "any"    # Or a specific model, e.g., "nvidia-a100-80gb"
              count: 1

        model:
          source:
            type: "huggingface"
            identifier: "mistralai/Mistral-7B-Instruct-v0.1"
            revision: "main"
          vllm:
            tensor_parallel_size: 1
            gpu_memory_utilization: 0.9
            trust_remote_code: true

        inference:
          system_prompt: |
            You are an expert editor. Improve the previous draft into a clear, accurate,
            and well-structured solution. Remove speculation and keep factual grounding.
          temperature: 0.6
          top_p: 0.95

        output:
          destination:
            type: "local"
            path: "/ignored/by/current-worker"
          artifacts:
            - "responses.json"
            - "logs"

    # ---------- Stage 3: Format output (produce strict JSON) ----------
    - name: "format-json"
      spec:
        resources:
          hardware:
            cpu: "8"
            memory: "16Gi"
            # CPU-only

        model:
          source:
            type: "huggingface"
            identifier: "meta-llama/Llama-3.2-1B-Instruct"
            revision: "main"
          transformers:
            device_map: "cpu"
            dtype: "float32"
            trust_remote_code: false
            low_cpu_mem_usage: true

        inference:
          system_prompt: |
            Convert the refined explanation into the following strict JSON:
            {
              "question": "<original question text>",
              "answer": "<final short answer>",
              "rationale": "<concise explanation>",
              "confidence": "<0.0-1.0>"
            }
            Output ONLY valid JSON with double-quoted keys/strings and no trailing commas.
          temperature: 0.2
          top_p: 0.9

        output:
          destination:
            type: "local"
            path: "./multi_step_cot"
          artifacts:
            - "responses.json"
            - "logs"
