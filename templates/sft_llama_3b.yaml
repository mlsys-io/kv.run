# sft_llama3b_deepspeed.yaml
apiVersion: mloc/v1
kind: SFTTask
metadata:
  name: llama-3b-sft-deepspeed

spec:
  taskType: "sft"

  resources:
    replicas: 1
    hardware:
      cpu: "8"
      memory: "32GiB"
      gpu:
        count: 2
        type: "any"

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-3B-Instruct"

  data:
    dataset_name: "openai/gsm8k"
    config_name: "main"
    split: "train[:5%]"
    prompt_column: "question"
    response_column: "answer"
    separator: "\n\nAnswer: "
    max_samples: 200

  training:
    # Enable multi-GPU execution; ``visible_devices`` limits torchrun to the listed GPU IDs.
    allow_multi_gpu: true
    visible_devices: "0,1"

    # Core hyperparameters controlled exclusively from this section.
    num_train_epochs: 2
    batch_size: 2
    gradient_accumulation_steps: 4
    learning_rate: 5.0e-5
    max_seq_length: 512
    logging_steps: 5
    save_steps: 50
    save_strategy: "steps"
    save_model: false  # enable to save locally and ship a zipped checkpoint when HTTP output is configured

    # Memory tuning.
    gradient_checkpointing: true
    packing: false
    fp16: false
    bf16: true

    # DeepSpeed stage-2 ZeRO configuration used by SFTExecutor when multi GPU is enabled.
    deepspeed:
      train_micro_batch_size_per_gpu: 2
      gradient_accumulation_steps: 4
      gradient_clipping: 1.0
      steps_per_print: 50
      zero_optimization:
        stage: 2
        overlap_comm: true
        contiguous_gradients: true
        reduce_bucket_size: 67108864
        allgather_bucket_size: 67108864
      bf16:
        enabled: true
      wall_clock_breakdown: false

  output:
    destination:
      type: "local"
      path: "./sft_llama_3b"
    artifacts:
      - "responses.json"
