apiVersion: mloc/v1
kind: InferenceTask
metadata:
  name: mistral-7b-infer
  owner: alice
  annotations:
    description: "Run text-generation inference using vLLM (Mistral-7B)"

spec:
  # New: scheduler only enables data-parallel when taskType=='inference'
  taskType: "inference"

  # New: data-parallel switch (set enabled: true to allow sharding)
  parallel:
    enabled: true           # false or remove to disable data-parallel
    max_shards: 8           # optional cap; scheduler also considers capacity/ratio

  # Resource requirements used by the orchestrator scheduler
  resources:
    replicas: 1
    hardware:
      cpu: "8"
      memory: "32Gi"
      gpu:
        type: "any"         # or a specific model, e.g. "nvidia-a100-80gb"
        count: 1

  # Model configuration and vLLM runtime options
  model:
    source:
      type: "huggingface"
      identifier: "mistralai/Mistral-7B-Instruct-v0.1"
      revision: "main"
    vllm:
      tensor_parallel_size: 1
      gpu_memory_utilization: 0.9
      trust_remote_code: true

  # Data resource (this split will be evenly partitioned across shards when parallel.enabled=true)
  data:
    type: "dataset"
    url: "openai/gsm8k"
    name: "main"
    split: "train[:1%]"     # scheduler will produce per-shard sub-splits like train[0%:0.5%], train[0.5%:1%], ...
    column: "question"
    shuffle: true
    seed: 42

  # Inference parameters (vLLM accepts max_tokens)
  inference:
    max_tokens: 128
    temperature: 0.7
    top_p: 0.95

  # Output configuration (current worker writes locally to RESULTS_DIR/<task_id>/responses.json)
  output:
    destination:
      type: "local"
      path: "./mistral_7b_infer"
    artifacts:
      - "responses.json"
      - "logs"
