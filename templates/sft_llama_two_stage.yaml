# sft_llama_two_stage.yaml
#
# Two-stage supervised fine-tuning pipeline that first adapts on GSM8K and then
# resumes training on OpenAI Humaneval using the checkpoint path produced by the
# first stage. Results are sent to the orchestrator via HTTP while remaining
# available under the worker's RESULTS_DIR.

apiVersion: mloc/v1
kind: SFTTask
metadata:
  name: llama-sft-two-stage

spec:
  taskType: "sft"

  resources:
    replicas: 1
    hardware:
      cpu: "8"
      memory: "32GiB"
      gpu:
        count: 1
        type: "any"

  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-1B-Instruct"

  output:
    destination:
      type: "http"
      url: "http://127.0.0.1:8080/api/v1/results"
      method: "POST"
      timeoutSec: 15
      headers:
        Authorization: "Bearer dev-token"
        Content-Type: "application/json"
    artifacts:
      - "responses.json"
      - "final_model"

  stages:
    - name: gsm8k-stage
      spec:
        data:
          dataset_name: "openai/gsm8k"
          config_name: "main"
          split: "train[:2%]"
          prompt_column: "question"
          response_column: "answer"
          separator: "\n\nAnswer: "
          max_samples: 120
        training:
          num_train_epochs: 1
          batch_size: 4
          gradient_accumulation_steps: 2
          learning_rate: 5.0e-5
          max_seq_length: 512
          logging_steps: 10
          save_steps: 50
          save_model: true
          gradient_checkpointing: false
          packing: false

    - name: humaneval-stage
      spec:
        checkpoint:
          load:
            type: "http"
            url: "${gsm8k-stage.result.final_model_archive_url}"
        data:
          dataset_name: "openai/openai_humaneval"
          split: "test"
          prompt_column: "prompt"
          response_column: "canonical_solution"
          separator: "\n\nSolution:\n"
          max_samples: 164
        training:
          num_train_epochs: 1
          batch_size: 2
          gradient_accumulation_steps: 4
          learning_rate: 3.0e-5
          max_seq_length: 768
          logging_steps: 10
          save_steps: 100
          save_model: true
          gradient_checkpointing: false
          packing: false
