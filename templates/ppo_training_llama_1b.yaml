apiVersion: mloc/v1
kind: TrainingTask
metadata:
  name: llama-1b-ppo-training
  owner: alice
  annotations:
    description: "PPO training for Llama-1B using reinforcement learning"

spec:
  taskType: "ppo"   # Worker will execute with the PPO module

  # Resource requirements used by the orchestrator scheduler
  resources:
    replicas: 1
    hardware:
      cpu: "16"
      memory: "64Gi"
      gpu:
        type: "any"       # or a specific model, e.g. "nvidia-a100-80gb"
        count: 1

  # Base model configuration
  model:
    source:
      type: "huggingface"
      identifier: "meta-llama/Llama-3.2-1B-Instruct"
      revision: "main"
      trust_remote_code: false
    config:
      fp16: true
      device_map_auto: true

  # Reward model configuration (optional)
  reward_model:
    identifier: "cardiffnlp/twitter-roberta-base-sentiment-latest"
    type: "sentiment"

  # Training dataset configuration
  data:
    # Option 1: Use Hugging Face dataset (recommended for production)
    # dataset_name: "Anthropic/hh-rlhf"
    # split: "train"
    # max_samples: 1000
    
    # Option 2: Use custom prompts (good for testing)
    prompts:
      - "Write a helpful and positive response to: How can I improve my productivity?"
      - "Create a motivational message for someone learning to code:"
      - "Explain quantum computing in an encouraging way:"
      - "Write a positive review for a local business:"
      - "Describe the benefits of exercise in an inspiring tone:"
      - "Give advice on maintaining work-life balance:"
      - "Explain the importance of continuous learning:"
      - "Write about the benefits of teamwork:"

  # PPO training parameters
  training:
    learning_rate: 1.41e-5
    batch_size: 4
    mini_batch_size: 1
    gradient_accumulation_steps: 4
    steps: 50                    # Reduced for faster demo
    ppo_epochs: 4
    target_kl: 0.1
    seed: 42
    early_stopping: false
    optimize_cuda_cache: true
    save_model: true
    save_freq: 25               # Save checkpoint every 25 steps
    log_with: null              # Options: "tensorboard", "wandb", null
    tracker_project_name: "mistral-ppo-training"

  # Text generation parameters during training
  generation:
    max_new_tokens: 256
    temperature: 0.7
    do_sample: true

  # Output configuration
  output:
    destination:
      type: "local"
      path: "./ppo_training_llama"
    artifacts:
      - "responses.json"
      - "logs"
      - "artifacts/checkpoints"
